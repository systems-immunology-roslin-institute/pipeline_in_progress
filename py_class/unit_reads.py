# The object contains the overall information.
# Unification is not inclued as part of the object, as different unification methods uses differenc child class
# The parent class "read_info" contains the overall required information, along with output methods
# - sample_annotation_obj : generated by unit_sample_annotation
# - gtf_obj : generated by unit_gtf
# - genename : current gene of interest
# - strnad : the strand (1 or -1 for positive and negative strand) the gene is on.
# - rnaseqtype : whether if the sequencing is single or paired end. This would effect the method for reading sam file

## Output format ##
# the output format of the script will be either
# - Output format 1
# - fsa (for blast input)
# - node annotation [ordered with node ID and node size being the first two column. The remaining columns according to a priority list]
import re, os, shutil
import py_class
import numpy as np
from py_class.unit_sam import read_sam
from py_class.unit_sam import read_sam_single
from py_class.unit_sam import read_sam_paired
from py_class.unit_bed import bed_unit
import py_fun
from py_fun.coord_comp import coord_str2lst
from py_fun.coord_comp import overlap
from py_fun.coord_comp import is_number
from py_fun.colour_select import graphia_colour
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.ticker import ScalarFormatter
import hashlib


colour_choices=["#E41A1C", "#AA3B50", "#705C83", "#377EB8", "#3E8E93", "#459E6E", "#4DAF4A", "#658E67", "#7E6E85", "#984EA3",  
                "#BA5E6C","#DC6E36", "#FF7F00", "#FFA910", "#FFD421", "#FFFF33", "#E1C62F", "#C38E2B", "#A65628", "#C1645A", 
                "#DB728C", "#F781BF", "#D789B2", "#B891A5", "#999999"] # a set of colours for plotting

# this base object use unique sequence as node
# the function self.print_output defines the processes that will be carried out to generate the output, which varies between unification types.
class read_unification(object):
    def __init__(self, sample_annotation_obj, genename, strand, chromosome, paired_read, unification):
        self.sample_annotation=sample_annotation_obj
        self.genename=genename
        self.strand=-1 if strand == 'r' else 1 
        self.paired_read=paired_read
        self.unification=unification.lower()
        self.unified_count={}
        self.unified_annotation_count={}
        self.special_char='[^\w -]'
        self.nodeid_num={}
        self.transcriptid_ordered=[]
        self.chromosome=chromosome
        self.read_count = 0
        self.read_avglen = 0
        self.start_end = []
        self.junctions={} # this records the number of times a junction is found in reads, an example of a junction is 123|234
        self.unified_annotation_order={'coord':70, 'seq':60, 'count':50,'count_':40, 'seq':30, 'merged_gtf':20, genename:5, 'merged_gtf_inter':10} # this roughly defines the order of output for node annotations, smaller first.
        # these are basic setting for different unification methods
        self.unification_key={'by_seq': 'seq', 'by_coord': 'coord', 'no_blast':'coord'}
        self.mapping_use={'by_seq': False, 'by_coord': True, 'no_blast': True}
        self.seq_fsa={'by_seq': True, 'by_coord': True, 'no_blast': False}
    def organise_readid_info(self):
        all_samples=list(self.sample_annotation.sample_info.keys())
        all_samples.sort()
        for current_sample in all_samples:
            current_bed_fp = self.sample_annotation.sample_info[current_sample]['bed_fp']
            current_sam_fp = self.sample_annotation.sample_info[current_sample]['sam_fp']
            current_annotation= self.sample_annotation.sample_info[current_sample]['annotation']
            current_annotation_idx = self.sample_annotation.group_order_idx[current_annotation]
            current_bed = bed_unit(current_bed_fp)
            current_read_info={} 
            with open(current_sam_fp, 'r') as open_sam:
                for line in open_sam:
                    if not re.match(r'^@', line): #ignore lines that start with @
                        # extract the information from the read ID and store the ID name so it wouldn't be counted multiple times
                        if self.paired_read:
                            current_sam_line = read_sam_paired(sam_line=line, strand=self.strand)
                        else:
                            current_sam_line = read_sam_single(line)
                        current_line_info = current_sam_line.read_seq_coord(current_bed)
                        if len(current_line_info) > 0:
                            current_readid, current_seq, current_coord = [current_line_info['readid'], current_line_info['seq'], current_line_info['coord']]
                            # urrent_read_info is for storing all the data for the current sam file using readid as key. 
                            # This is to ensure identification of reads with multiple alignments
                            # The values are stored in an array ['sequence', 'coordinates'] 
                            if current_readid not in current_read_info:
                                current_read_info[current_readid]={'seq':[current_seq],'coord':current_coord} 
                            else:
                                current_read_info[current_readid]['seq'].append(current_seq) if current_seq not in current_read_info[current_readid]['seq'] else 0
                                current_read_info[current_readid]['coord'].extend(current_coord) # current_coord is a list of strings, each string being an unique address
                            # Reacord any junction reads. This is used to characterise the merged gtf later
                            if '|' in current_coord:
                                current_coord_split= current_coord.split("-")
                                for val in current_coord_split:
                                    if '|' in val:
                                        self.junctions[val]=0 if val not in self.junctions else 0
                                        self.junctions[val] += 1
            self.readid2uniquekey(read_info=current_read_info, annot_idx=current_annotation_idx)
            self.read_count=len(current_read_info) + self.read_count
        self.read_avglen=(self.read_avglen/self.read_count) # this is the average length of reads across all samples
        self.read_count=self.read_count/len(all_samples) # this is the average read count per sample
        # Go through each read and calculate average per sample (i.e. divide each value by the group size)
        # Convert the saved lists into np.array
        sample_group_ordered=[key_pair[0] for key_pair in sorted(self.sample_annotation.group_order_idx.items(), key=lambda x: (x[1],x[0]))] 
        group_size = np.array([self.sample_annotation.group_size[grp_name] for grp_name in sample_group_ordered])
        for key, count_list in self.unified_count.items():
            averaged_count = np.array(count_list)/group_size
            self.unified_count[key] = averaged_count
    # This is used to turn the readid to unique key, discarding reads that have been mapped to multiple locations within the same gene
    def readid2uniquekey(self, read_info, annot_idx):
        val_as_key = self.unification_key[self.unification] # this is the value that will be used as key (such as 'seq' or 'coord')
        node_annotation=[val for val in ['seq', 'coord'] if val != val_as_key] # the other information (such as 'seq' or 'coord')
        for current_readid, current_read_info in read_info.items():
            self.read_avglen = self.read_avglen+ len(current_read_info['seq'][0])
            # If the unification does not use alignment coordinates (therefore multipmappers are ok)
            # or if the readid only has one mapped location
            if (not self.mapping_use[self.unification]) or (len(current_read_info['coord'])==1):
                # Pick the longest for the unified key as it would be either read sequence with the least amount of soft clipping or just one value when coordinates are used for mapping
                if len(current_read_info[val_as_key])>1: # this if statement will only be met in unification by sequence (i.e. self.mapping_use[self.unification] is False)
                    unified_key=max(current_read_info[val_as_key], key=len) 
                else: # otherwise take the only value as the key
                    unified_key=current_read_info[val_as_key][0]
                # Use the uniqifed key to count
                if unified_key not in self.unified_count:
                    self.unified_count[unified_key]=[0]*len(self.sample_annotation.group_order_idx) # note that this is saved as an integer, and if divisions are required later on, will need to be convereted to float first
                    self.unified_annotation_count[unified_key]={}
                self.unified_count[unified_key][annot_idx] = self.unified_count[unified_key][annot_idx] + 1
                # Node annotation is made at an overall level (i.e. across all samples. The number of times each annotation has been made for a node is tracked separatly)
                for annot_type in node_annotation:
                    annot_val=current_read_info[annot_type]
                    if type(annot_val) is list:
                        #### this makes the annotation value into a string
                        annot_val=",".join(sorted(annot_val))
                    self.unified_annotation_count[unified_key].update({annot_type:{}}) if annot_type not in self.unified_annotation_count[unified_key] else 0
                    self.unified_annotation_count[unified_key][annot_type].update({annot_val:0}) if annot_val not in self.unified_annotation_count[unified_key][annot_type] else 0
                    self.unified_annotation_count[unified_key][annot_type][annot_val] +=1
    def read_defined_segments(self, gtf_obj):
        # Define chromosomal positions covered by gtf transcript model or read
        gtf_covered_position = gtf_obj.gtf_covered_positions
        # The first step is to determine the frequency of occurances for each junction
        # Junctions are saved as the lower coordinate
        junction_info = self.get_breakpoints()
        self.start_end = [min(list(gtf_covered_position.keys())), max(list(gtf_covered_position.keys()))]
        segment_out = self.get_segment(position_start = self.start_end[0], position_end=self.start_end[1], position_covered=gtf_covered_position, segment_end=junction_info['end'], segment_start=junction_info['start'])
        return(segment_out)
    def get_breakpoints(self, keep_read_positions = False):
        # Settings 
        breakpoint_times_threshold = 5 # the minimum number of times a breakpoints must occur within at least one group for the breakpoint to be considered true
        # Find out the positions where there is a junction and their counts across groups
        # Find out the positions covered by at least one read and record their frequency. Only do this for positions not covered by .gtf files.
        read_covered_position = {}        
        all_breakpoints_end={}
        all_breakpoints_start={}
        for current_key in self.unified_annotation_count:
            # if the unification method involves using coordinates, the key is used as it would be the coordinates. Otherwise, the coord information will be in self.unified_annotation_count
            if self.unification_key[self.unification] == 'coord':
                coord_set = current_key  
                current_coord_set_lst=coord_str2lst(coord_set)
            else:
                coord_set=[key_pair[0] for key_pair in sorted(self.unified_annotation_count[current_key]['coord'].items(), key=lambda x: (x[1],x[0]), reverse=True)][0]
                coord_set=sorted((coord_set.split(",")))[0]
                current_coord_set_lst=coord_str2lst(coord_set)
            for current_coordset in current_coord_set_lst:
                # only record the number of times each position is covered if it is for labelling segments in the no-blast method
                # Unnecessary for when trying to work out merged gtf
                if keep_read_positions:
                    for position in range(current_coordset[0], (current_coordset[1] +1 )):
                        read_covered_position[position]=self.unified_count[current_key] if position not in read_covered_position else read_covered_position[position] + self.unified_count[current_key]
            if len(current_coord_set_lst) > 1:
                # find positions for either side of the junction, always only recording the position before the break points
                for idx in range(1, len(current_coord_set_lst)):
                    previous_idx = int(idx)-1
                    before_junction=current_coord_set_lst[previous_idx][1]
                    after_junction=current_coord_set_lst[idx][0] 
                    all_breakpoints_end[before_junction] = self.unified_count[current_key] if before_junction not in all_breakpoints_end else all_breakpoints_end[before_junction] + self.unified_count[current_key]
                    all_breakpoints_start[after_junction] = self.unified_count[current_key] if after_junction not in all_breakpoints_start else all_breakpoints_start[after_junction] + self.unified_count[current_key]
        # Check if the breakpoint has occurred more than x number of times in at least 1 group, where x is breakpoint_times_threshold
        breakpoints_final_end = {} # the breakpoints that will be used in the geneation of nodes
        for current_breakpoint, breakpoint_count in all_breakpoints_end.items():
            if max(breakpoint_count) >= breakpoint_times_threshold:
                breakpoints_final_end[current_breakpoint]= max(breakpoint_count)
        breakpoints_final_start = {} # the breakpoints that will be used in the geneation of nodes
        for current_breakpoint, breakpoint_count in all_breakpoints_start.items():
            if max(breakpoint_count) >= breakpoint_times_threshold:
                breakpoints_final_start[current_breakpoint]= max(breakpoint_count)
        return({'end' : breakpoints_final_end, 'start':breakpoints_final_start, 'position_covered':read_covered_position})
    # This function is used to detine the segments. 
    # For instance, merged annotation uses positions covered by GTF files + junctions within the reads to generate segments
    def get_segment(self, position_start, position_end, position_covered, segment_end, segment_start, coverage_threshold = 0):
        segment_threshold = 3        
        segment_all=[]
        current_start = position_start
        # break up the segments
        for position in range(position_start, position_end+1):
            if position in position_covered:
                max_count = max(position_covered[position]) if type(position_covered[position]) is np.ndarray else position_covered[position]
            else:
                max_count = 0
            if (position not in position_covered or (max_count < coverage_threshold)) and current_start:
                segment_all.append([current_start,position-1])
                current_start=None
            elif position in position_covered and (position in segment_end or position==position_end) and current_start:
                segment_all.append([current_start,position])
                current_start=None
            elif position in position_covered and (position in segment_start or (position in position_covered and max_count >= coverage_threshold)) and not current_start:
                current_start=position
            else:
                pass
        # Merge segments with previous segments if it is < 5bp and touching another exon. Merge towards the least breakpoints.
        changes_in_current_cycle = False
        all_clear = False
        cycle_count=0
        current_segments=[]
        def merge_with_previous(passed_segments, all_segments, current_index):
            passed_segments[-1][1]=all_segments[current_index][1]
            return([True, passed_segments])
        def merge_with_next(passed_segments, all_segments, current_index):
            passed_segments.append([all_segments[current_index][0], all_segments[current_index+1][1]])
            return([True, True, passed_segments])
        while not all_clear:
            changes_in_current_cycle = False
            skip_this_segment = False
            for segment_idx, segment in enumerate(segment_all):
                if skip_this_segment:
                    skip_this_segment = False
                    continue
                if (segment[1]-segment[0]+1) < segment_threshold:
                    if segment_idx ==0:
                        if segment[1] + 1 == segment_all[segment_idx+1][0]:
                            changes_in_current_cycle, skip_this_segment, current_segments = merge_with_next(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                    elif segment_idx == (len(segment_all)-1):
                        if segment[0] - 1 == segment_all[segment_idx-1][1]:
                            changes_in_current_cycle, current_segments = merge_with_previous(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                    # if it's only touching with the previous segment 
                    elif segment[0] - 1 == segment_all[segment_idx-1][1] and segment[1] + 1 != segment_all[segment_idx+1][0]:
                        changes_in_current_cycle, current_segments = merge_with_previous(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)                   
                    # if it's only touching with the next segment
                    elif segment[0] - 1 != segment_all[segment_idx-1][1] and segment[1] + 1 == segment_all[segment_idx+1][0]:
                        changes_in_current_cycle, skip_this_segment, current_segments = merge_with_next(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                    # if it's touching both the previous and the next segment, merge with one with lower counts in the breakpoint.
                    elif segment[0] - 1 == segment_all[segment_idx-1][1] and segment[1] + 1 == segment_all[segment_idx+1][0]:                    
                        breakpoint_prev=segment_start[segment[0]] if segment[0] in segment_start else None
                        breakpoint_next=segment_end[segment[1]] if segment[1] in segment_end else None
                        if breakpoint_prev and breakpoint_next:
                            if breakpoint_prev > breakpoint_next:
                                changes_in_current_cycle, skip_this_segment, current_segments = merge_with_next(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                            else:
                                changes_in_current_cycle, current_segments = merge_with_previous(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                        elif breakpoint_prev:
                            changes_in_current_cycle, skip_this_segment, current_segments = merge_with_next(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                        elif breakpoint_next:
                            changes_in_current_cycle, current_segments = merge_with_previous(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                        # if breakpoints are absent and cannot be used to assess best merge, just merge with previous
                        else:
                            changes_in_current_cycle, skip_this_segment, current_segments = merge_with_next(passed_segments=current_segments, all_segments=segment_all, current_index=segment_idx)
                    # just append current segment
                    else:
                        current_segments.append(segment)
                else:
                    current_segments.append(segment)
            if not changes_in_current_cycle:
                all_clear = True
            cycle_count+=1
            if cycle_count > 1000: # stop the process if there had been more than 1000 cycles
                all_clear=True
            segment_all = current_segments
            current_segments = []
        
        return(segment_all)
    def print_fsa(self,cache_dir):
        out_fp=os.path.join(cache_dir, "%s/fsa/%s.fsa" %(self.genename, self.genename))
        open_out=open(out_fp, 'w+')
        for current_nodeid in self.nodeid_num: 
            current_nodeid_num=self.nodeid_num[current_nodeid]
            if self.unification_key[self.unification] == 'seq':
                seq=current_nodeid
            else:
                seq=self.unified_annotation_count[current_nodeid]['seq']
                seq=[key_pair[0] for key_pair in sorted(seq.items(), key=lambda x: (x[1],len(x[0]),x[0]), reverse=True)][0] # return the sequence with the most count, use the sequence with the longest sequence on tie break                
            outline=">node%d\n%s\n"%(current_nodeid_num, seq)   
            open_out.write(outline)
        open_out.close()
        
    def print_node_annotation(self,cache_dir):
        # open output folder and put down header for column names
        out_fp=os.path.join(cache_dir, "%s/node/%s_node_annotation.txt" %(self.genename, self.genename))
        open_out=open(out_fp, 'w+')
        headerline = ['nodeid_num', 'nodeid']
        headerline.append('total_count') if len(self.sample_annotation.group_order_idx) > 1 else 0 # create total count if there is more than one group
        sample_group_ordered=[key_pair[0] for key_pair in sorted(self.sample_annotation.group_order_idx.items(), key=lambda x: (x[1],x[0]))] 
        headerline.extend([str(val) for val in sample_group_ordered])
        # print the sample (group) order according to its index, these are the columns for count
        # determine the order for annotation using the node in annotation     
        annotation_order={}
        first_node_id=list(self.unified_annotation_count.keys())[0]
        for annot_type in self.unified_annotation_count[first_node_id]: # check the annotation types using the last node
            current_annotation_order=[]
            for key, order_val in self.unified_annotation_order.items():
                if key in annot_type.lower():
                    current_annotation_order.append(order_val)
            current_annotation_order=sum(current_annotation_order) if len(current_annotation_order) > 0 else 0
            annotation_order.update({annot_type:current_annotation_order})
        annotation_order = [key_pair[0] for key_pair in sorted(annotation_order.items(), key=lambda x: (x[1],x[0]), reverse=True)]
        annotation_order_out=[str(val) for val in annotation_order]
        headerline.extend(annotation_order_out)
        # make sure none of the header values starts with a number or contains any special character
        headerline=[''.join([e for e in val if e == '_' or e.isalnum()]) for val in headerline] # replace any special characters
        headerline=[val if not val[0].isdigit() else 'X'+val for val in headerline] # if an annotation starts with a digit, add a captial X to it. This is the same convention as R
        headerline= "\t".join(headerline)
        open_out.write(headerline + "\n")
        # Go through each node and write node information in file
        node_id_num=1
        for current_nodeid in self.unified_count:
            # first value in the outline are always nodeid
            self.nodeid_num[current_nodeid]=node_id_num
            outline=["node%d\t%s"%(node_id_num, str(current_nodeid))]
            node_id_num+=1
            # The 2nd to xth column are detials on the number of reads a node contains
            # In the case of no-blast/purely coordinates based method, 
            # This value would be the frequency a base within the node has been sequenced 
            # ie. if there is 2 reads that overlaps a node with a length of 15 bases, one with 7 base overlap and the other with 8 base overlap, the value would be (7+8)/15, or 1 per base
            # As the count data is stored in a vector of the order and length of self.sample_annotation.group_order_val
            # the corresponding values need to be printed to separate columns
            # if this is no_blast, divide the count value by the length of the node
            count_total = "%.2f"%(self.unified_count[current_nodeid].sum()) # create total count if there is more than one group
            count_group=(self.unified_count[current_nodeid]).tolist() # this is the average count for each group
            count_str = "\t".join(["%.2f"%(val) for val in count_group])
            outline.append(count_total) if len(self.sample_annotation.group_order_idx) > 1 else 0 # create total count if there is more than one group
            outline.append(count_str)
            # Print out the annotation in an ordered manner. 
            # As the content of each annotation has been stored in a hash, with key:value = annotation_val:count,
            # Only the annotation with the highest count is used
            node_annot_all=self.unified_annotation_count[current_nodeid] # this is a dictionary with {annotationA: {annot_val:count}, annotationB:{annot_val:count, annotval:count}} ..etc
            for annot_type in annotation_order: # annotation_order indciate the order in which annotation occur (for instance, in the example above, it indicates the order that annotationA and annotationB columns)
                annot_detail=node_annot_all[annot_type] # this is a dictionary
                # reorder the annotation detail with the highest count
                annot_detail_keyhighcount=str([key_pair[0] for key_pair in sorted(annot_detail.items(), key=lambda x: (x[1],x[0]), reverse=True)][0]) if len(annot_detail) > 0  else "" # this is the key with the highest count
                #annot_detail_keyall=list(annot_detail.keys()) # this is a list of all the keys
                outline.append(annot_detail_keyhighcount)
            outline=[str(val) for val in outline]
            outline="\t".join(outline)
            open_out.write(outline + "\n")
        open_out.close()
    # Use the transcript model information in gtf_object annotate each unique_id
    def annotate_transcript_model(self, gtf_obj):
        self.transcriptid_ordered=gtf_obj.transcriptid_ordered
        for current_nodeid in self.unified_annotation_count:
            # Use the key as coordinates if the key is based on coordinates
            if self.unification_key[self.unification] == 'coord':
                current_coord=current_nodeid
            else:
                current_coord=[key_pair[0] for key_pair in sorted(self.unified_annotation_count[current_nodeid]['coord'].items(), key=lambda x: (x[1],x[0]), reverse=True)][0]
                current_coord=sorted((current_coord.split(",")))[0]
            # Go through each transcript model and check the amount of overlap with each exon
            gtf_annotation=gtf_obj.compare_coord_gtf(coord_in=current_coord)
            self.unified_annotation_count[current_nodeid].update(gtf_annotation)
    # This function takes in the cache folder and prints out the output files
    # This print_output is specified for 'by_seq' unification
    def print_output(self, gtf_obj, cache_dir, out_dir):
        self.annotate_transcript_model(gtf_obj = gtf_obj)
        self.print_node_annotation(cache_dir=cache_dir)
        self.print_fsa(cache_dir=cache_dir)

# This child class is for unification that uses coordinates to define edges
class read_unification_no_blast(read_unification):
    def __init__(self, *args, **kwargs):
        super(read_unification_no_blast, self).__init__(*args, **kwargs)
        self.edge_count={}
    def make_no_blast_network(self, gtf_obj):
        # The merged annotation generated from gtf and read coverage will be split into smaller fragments, each fragment referred to as a node
        # Setting
        node_size = 50  # the number of basepairs for each node (those smaller than half this value will be merged with the previous node)
        nodes_defined=[]
        # Go through each defined segment
        breakpoints_coverage = self.get_breakpoints(keep_read_positions=True)
        start_end = self.start_end
        start_end.sort()
        read_segments = self.get_segment(position_start=start_end[0], position_end=start_end[1], position_covered=breakpoints_coverage['position_covered'], segment_end =breakpoints_coverage['end'] , segment_start=breakpoints_coverage['start'], coverage_threshold = 5)
        read_segments.sort()
        for segment in read_segments:
            passed_segments=[]
            segment.sort()
            node_start=segment[0]
            end_reached = False
            while not end_reached:
                node_end= node_start + node_size-1
                if node_end >= segment[1]:
                    node_end = segment[1]
                    end_reached = True
                # append the node definition to nodes_defined if the size of this definition is larger than half of the node size setting
                if (node_end - node_start + 1) >= (0.5 * node_size):
                    passed_segments.append([node_start, node_end])
                    node_start=node_end+1
                elif len(passed_segments) > 0:
                    passed_segments[-1][-1]=node_end
                    node_start=node_end
                else:
                    passed_segments.append(segment)
            nodes_defined.extend(passed_segments)
        # Go through coord_unified summary to determine the count for each of the defined nodes
        node_count = {}
        node_annotation_count = {} # in the following process, the counts are saved as a list, each element of the list reflect the counts in a group
        edge_count={}
        nodes_defined.sort()
        for unique_coord, current_count in self.unified_count.items():
            positions = coord_str2lst(unique_coord)
            # check through overlaps between each node
            previous_read_segment_node = None
            for current_coordset in positions:
                current_length= current_coordset[1]-current_coordset[0] + 1
                position_set=set(list(range(current_coordset[0], current_coordset[1]+1))) # these are positions covered by the current segment of the read
                accounted_length=0
                matched_node=None
                for node in nodes_defined:
                    overlap_len = len(position_set.intersection(set(range(node[0], node[1]+1))))
                    if overlap_len > 0:
                        accounted_length = accounted_length + overlap_len
                        node_name = "%d-%d"%(node[0], node[1])
                        node_count[node_name] = (current_count*overlap_len) if node_name not in node_count else node_count[node_name] + (current_count*overlap_len)
                        node_annotation_count[node_name]={}
                        node_pair=None
                        if previous_read_segment_node and (not matched_node):
                            node_pair = [previous_read_segment_node, node_name]
                        elif matched_node:
                            node_pair = [matched_node, node_name]
                        else:
                            pass
                        if node_pair:
                            edge_count.update({node_pair[0]:{}}) if node_pair[0] not in edge_count else 0
                            if node_pair[1] not in edge_count[node_pair[0]]:
                                edge_count[node_pair[0]].update({node_pair[1]:current_count}) 
                            else:
                                edge_count[node_pair[0]][node_pair[1]] = edge_count[node_pair[0]][node_pair[1]] + current_count
                        matched_node = node_name
                    if accounted_length >= current_length:
                        previous_read_segment_node = node_name
                        break # break if the full length has been accounted for
        # Remove edges that are covered less than 20 reads???
        # convert np.array back to lists
        for node in node_count:
            node_num = coord_str2lst(node)
            node_len = node_num[0][1] - node_num[0][0] + 1          
            node_count[node] = (node_count[node]/node_len)
        self.unified_count = node_count
        self.edge_count = edge_count
        self.unified_annotation_count = node_annotation_count
    # Remove intronic nodes
    # this process also removes nodes that do not have any edges
    def remove_excess_nodes(self):
        edge_threshold = 5 # an edge must be covered by this % of the reads
        exonic_node_edge = {} # this stores the sum of maximum edge of each exonic node
        intronic_node_edge = {} # this stores the sum of maximum edge of each intronic node
        exonic_nodes = {}
        for node, node_annotation in self.unified_annotation_count.items():
            if (max([len(transcript_id_annotation) for transcript_id_annotation in list(node_annotation.values())])) > 0:
                exonic_nodes[node] = 1
        # Determine the maximum edge of each exonic node
        for node1 in self.edge_count:
            for node2, edges in self.edge_count[node1].items():
                if node1 in exonic_nodes and node2 in exonic_nodes:
                    exonic_node_edge.update({node1:0}) if node1 not in exonic_node_edge else 0
                    exonic_node_edge.update({node2:0}) if node2 not in exonic_node_edge else 0
                    exonic_node_edge[node1] += max(edges)
                    exonic_node_edge[node2] += max(edges)
                else:
                    # record the maximum edge (across groups) attached to each of the intron
                    if node1 not in exonic_nodes:
                        intronic_node_edge.update({node1:0}) if node1 not in intronic_node_edge else 0
                        intronic_node_edge[node1] = max(edges)
                    if node2 not in exonic_nodes:
                        intronic_node_edge.update({node2:0}) if node2 not in intronic_node_edge else 0
                        intronic_node_edge[node2] += max(edges)
        # For an intronic node to be retained, it must have edges higher than the median exonic edge (maximum of the group) divided by 2
        intronic_edge_threshold = np.median(np.array(list(exonic_node_edge.values()))) / 4
        excess_node_removed_count = {}
        excess_node_removed_annotation = {}
        for node in self.unified_count:
            if ((node in exonic_node_edge) and exonic_node_edge[node] > edge_threshold) or ((node in intronic_node_edge) and (intronic_node_edge[node] > intronic_edge_threshold)):
                excess_node_removed_count[node] = self.unified_count[node]
                excess_node_removed_annotation[node] = self.unified_annotation_count[node]
        self.unified_count = excess_node_removed_count
        self.unified_annotation_count = excess_node_removed_annotation
    # this reads in the printed node annotation
    # and prints out a graphml format output, adding edge information
    def print_gml(self, cache_dir, out_dir):
        node_annotation_fp=os.path.join(cache_dir, "%s/node/%s_node_annotation.txt" %(self.genename, self.genename))
        #gml_dir=os.path.join(out_dir, cache_dir.split("/")[-1])
        gml_dir=os.path.join(cache_dir, "gml")
        gml_fp= os.path.join(gml_dir, "%s_%s.gml" %(self.genename, self.unification))
        if not os.path.exists(gml_dir):
            os.makedirs(gml_dir)
        node_info={}
        node_id2name={}
        with open(node_annotation_fp, 'r') as node_annotation:
            for line_num, line in enumerate(node_annotation):
                line=line.rstrip('\r\n')
                line=line.split('\t')
                if line_num == 0:
                    annotation_header=line
                else:
                    current_node_info={}
                    for idx, val in enumerate(line):
                        current_node_info[annotation_header[idx]] = val
                    current_node_name=current_node_info['nodeid']
                    current_node_info['nodeid_num']=current_node_info['nodeid_num'].replace("node", "")
                    node_info[current_node_name]=(current_node_info)  
                    node_id2name[int(current_node_info['nodeid_num'])]=current_node_name
        gml = open(gml_fp, 'w+')
        gml.write("graph[\n")
        sample_group_ordered=[key_pair[0] for key_pair in sorted(self.sample_annotation.group_order_idx.items(), key=lambda x: (x[1],x[0]))] 
        sample_group_ordered_outname=[''.join([e for e in val if e == '_' or e.isalnum()]) for val in sample_group_ordered] # replace any special characters
        
        for node_numid in sorted(node_id2name):
            node_name=node_id2name[node_numid]
            # column 1 and 2 are always nodeid_num and nodeid
            # everything else is annotation
            # Replace any special characters
            gml.write("  node [\n")
            gml.write("    id %d\n" %(node_numid))
            gml.write("    label \"%s:%s\"\n"%(self.chromosome, node_name))
            for idx in range(2,len(annotation_header)):
                attribute_val=node_info[node_name][annotation_header[idx]]
                if is_number(attribute_val):
                    attribute_val=float(attribute_val)
                else:
                    attribute_val="\"%s\""%(attribute_val)
                gml.write("    %s %s\n"%(annotation_header[idx], attribute_val))
            gml.write("  ]\n")
        # print out edge count
        for node1 in self.edge_count:
            for node2, count_array in self.edge_count[node1].items():
                if max(count_array) > 3:
                    if (node1 in node_info) and (node2 in node_info):
                        gml.write("  edge [\n")
                        gml.write("    source %s\n" %(node_info[node1]['nodeid_num']))
                        gml.write("    target %s\n" %(node_info[node2]['nodeid_num']))
                        all_val=[]
                        for grp_idx, sample_grp in enumerate(sample_group_ordered):
                            # only record the edge if the maxiumum is higher than 3
                            output_val = count_array[grp_idx]
                            gml.write("    %s %.2f\n" %(sample_group_ordered_outname[grp_idx], output_val))
                            all_val.append(output_val)
                        if len(sample_group_ordered) > 1:
                            gml.write("    _total %.2f\n" %(sum(all_val)/len(sample_group_ordered)))
                            gml.write("    _max %.2f\n" %(max(all_val)))
                        gml.write("  ]\n")
        gml.write("]\n")
        gml.close()
    # Print out sample level node details for statistical analysis
    def print_sample_level_node_count(self, cache_dir, out_dir):
        all_samples=list(self.sample_annotation.sample_info.keys())
        all_samples.sort()
        node_annotation_fp=os.path.join(cache_dir, "%s/node/%s_node_annotation.txt" %(self.genename, self.genename))
        recorded_node_count={}
        recorded_node_coord={}
        total_read_count_per_sample=[0.0]*len(all_samples)
        with open(node_annotation_fp, 'r') as node_annotation:
            for line_num, line in enumerate(node_annotation):
                line=line.rstrip('\r\n')
                line=line.split('\t')
                if line_num == 0:
                    annotation_header=line
                    merged_annotation_idx=[idx for idx, val in enumerate(annotation_header) if val=="_merged_annotation"][0]
                    nodeid_idx=[idx for idx, val in enumerate(annotation_header) if val=="nodeid"][0]
                else:
                    if line[merged_annotation_idx] != "":
                        recorded_node_count[line[nodeid_idx]]=[0.0]*len(all_samples)
                        current_range=[int(val) for val in line[nodeid_idx].split("-")]
                        recorded_node_coord[line[nodeid_idx]]={coord:0 for coord in list(range(current_range[0], (current_range[1]+1)))} # store this as a hash of coordinates
        # Go through each sample and record the overlap with each node
        for current_sample_idx, current_sample in enumerate(all_samples):
            current_bed_fp = self.sample_annotation.sample_info[current_sample]['bed_fp']
            current_sam_fp = self.sample_annotation.sample_info[current_sample]['sam_fp']
            current_bed = bed_unit(current_bed_fp)
            read_info={} 
            with open(current_sam_fp, 'r') as open_sam:
                for line in open_sam:
                    if not re.match(r'^@', line): #ignore lines that start with @
                        # extract the information from the read ID and store the ID name so it wouldn't be counted multiple times
                        if self.paired_read:
                            current_sam_line = read_sam_paired(sam_line=line, strand=self.strand)
                        else:
                            current_sam_line = read_sam_single(line)
                        current_line_info = current_sam_line.read_seq_coord(current_bed)
                        if len(current_line_info) > 0:
                            current_readid, current_coord = [current_line_info['readid'], current_line_info['coord']]
                            # urrent_read_info is for storing all the data for the current sam file using readid as key. 
                            # This is to ensure identification of reads with multiple alignments
                            # The values are stored in an array ['sequence', 'coordinates'] 
                            if current_readid not in read_info:
                                read_info[current_readid]=current_coord
                            else:
                                read_info[current_readid].extend(current_coord) # current_coord is a list of strings, each string being an unique address
            for current_readid, current_read_info in read_info.items():
                # If the unification does not use alignment coordinates (therefore multipmappers are ok)
                # or if the readid only has one mapped location
                if len(current_read_info)==1:
                    coord_lst=coord_str2lst(current_read_info[0])
                    current_read_position={val:0 for coordset in coord_lst for val in range(coordset[0],(coordset[1] + 1))}
                    accounted_len=0
                    for node, node_position in recorded_node_coord.items():
                        intersection = len(set(node_position.keys()) & set(current_read_position.keys()))
                        if intersection>0:
                            recorded_node_count[node][current_sample_idx]+=intersection/len(current_read_position)
                            total_read_count_per_sample[current_sample_idx]+=intersection/len(current_read_position)
                        accounted_len=accounted_len + intersection
                        if accounted_len >= len(current_read_position):
                            break
        # Print out values
        sample_level_node_count_fp = os.path.join(cache_dir, "%s/node/%s_node_value_per_sample.txt" %(self.genename, self.genename))
        sample_level_node_count = open(sample_level_node_count_fp, 'w+')
        sample_level_node_count.write("\t".join(["sample"]+all_samples))
        node_variance={}
        for node in recorded_node_count:
            current_values=[node]+["%.2f"%(val) for val in recorded_node_count[node]]
            current_portion=[]
            for i in range(0,len(recorded_node_count[node])):
                current_portion = recorded_node_count[node][i]/total_read_count_per_sample[i]*100 if total_read_count_per_sample[i] > 0 else 0
            node_variance[node]=np.var(np.array(current_portion))
            sample_level_node_count.write("\t".join(current_values)+"\n")
        sample_level_node_count.close()
        
        # Add the node variance to the node_annotation
        node_annotation_fp2=os.path.join(cache_dir, "%s/node/%s_node_annotation2.txt" %(self.genename, self.genename))
        node_annotation2=open(node_annotation_fp2, 'w+')
        with open(node_annotation_fp, 'r') as node_annotation:
            for line_num, line in enumerate(node_annotation):
                line=line.rstrip('\r\n')
                line=line.split('\t')
                if line_num == 0:
                    annotation_header=line[0:3] + ['variance'] + line[3:len(line)]
                    node_annotation2.write("\t".join(annotation_header) + "\n")
                else:
                    if line[1] in node_variance:
                        current_line = line[0:3] + [str(node_variance[line[1]])] + line[3:len(line)]
                    else:
                        current_line = line[0:3] + [""] + line[3:len(line)]
                    node_annotation2.write("\t".join(current_line) + "\n")
        node_annotation2.close()
        shutil.copy(node_annotation_fp2, node_annotation_fp)
        os.remove(node_annotation_fp2)
    # Print merged_annotation and levels in each sample
    def print_matplotlib_graph(self, cache_dir, gtf_obj, overlay=True):
        # print the merged_annotation model
        output_fp=os.path.join(cache_dir, "%s/gtf/%s_merged_annotation.png" %(self.genename, self.genename))
        merged_annotation = gtf_obj.annotation_by_transcriptid['_merged_annotation'].exon_dict
        lowest_val = gtf_obj.min
        highest_val = gtf_obj.max
        blocks=[]
        block_names=[]
        colours=[]
        current_block=[lowest_val,None]
        prev_exon=merged_annotation[lowest_val]
        for i in range(lowest_val, highest_val+2):
            if (i not in merged_annotation) or (prev_exon and prev_exon!=merged_annotation[i]):
                if current_block[0]:
                    current_block[1]=int(i)-1
                    blocks.append(current_block)
                    current_colour=graphia_colour(prev_exon)
                    #current_legend = "%s (%s-%s; %dbp)"%(prev_exon, format(current_block[0],",d"), format(current_block[1],",d"), current_block[1]-current_block[0] + 1)
                    current_legend = "%s"%(prev_exon)
                    block_names.append(current_legend)
                    colours.append(current_colour)
                    if i in merged_annotation:
                        current_block=[int(i),None]
                        prev_exon=merged_annotation[i]
                    else:
                        current_block=[None,None]
                        prev_exon = None
            elif i in merged_annotation and (not current_block[0]):
                current_block[0]=int(i)
                prev_exon=merged_annotation[i]
        # Find out the number of samples/groups
        sample_group_ordered=[key_pair[0] for key_pair in sorted(self.sample_annotation.group_order_idx.items(), key=lambda x: (x[1],x[0]))] 
        number_of_groups = len(sample_group_ordered)
        # The length of the figure is fixed at 15, height is 3 per sample, + 5 margin and 1.5 for annotation
        fig_height= 2 + number_of_groups*2 + 1
        # The bottom (ie. merged annotation) will be roughly 1/2 of the group/sample figures
        fig, axarr = plt.subplots(number_of_groups + 1, sharex=True, figsize=(15,fig_height), gridspec_kw = {'height_ratios':[3]*number_of_groups + [2]})
        axarr[-2].set_xlim([(lowest_val - (highest_val-lowest_val)/25),(highest_val + (highest_val-lowest_val)/25)])
        for idx, current_block in enumerate(blocks):
            axarr[-1].add_patch(
            patches.Rectangle((current_block[0],0.1),   # this is the x/y coordinates of the bottom left corner
                current_block[1]-current_block[0], # width of the box
                0.05, # height of the box
                #alpha=0.3, # transparency
                linestyle=None, 
                color=colours[idx])
            )
            block_text=block_names[idx].replace("e", "")
            block_text=re.sub("^0+","",block_text)
            matplotlib.pyplot.text((current_block[0]+current_block[1])/2, 0.05, block_text, horizontalalignment='center', verticalalignment='center', )
        axarr[-1].set_ylim([0, 0.2])
        axarr[-1].xaxis.set_ticks_position('none') 
        #axarr[1].legend(all_patches, block_names, loc='lower right')
        #box = axarr[0].get_position()
        #axarr[0].set_position([box.x0, box.y0, box.width * 0.8, box.height])
        #axarr[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))
        
        # Add in node value for each group
        # if mapping
        position_counts={}
        for node, current_count in self.unified_count.items():
            if self.mapping_use:
                coord_set=coord_str2lst(node)
            else:
                coord_set=[key_pair[0] for key_pair in sorted(self.unified_annotation_count[node]['coord'].items(), key=lambda x: (x[1],x[0]), reverse=True)][0]
                coord_set=sorted((coord_set.split(",")))[0]
                coord_set=coord_str2lst(coord_set)
            for current_coord_set in coord_set:
                for i in range(current_coord_set[0], (current_coord_set[1]+1)):
                    position_counts[i]=current_count if i not in position_counts else position_counts[i] + current_count
        # Print out count for each group
        #group_size = np.array([self.sample_annotation.group_size[grp_name] for grp_name in sample_group_ordered])
        # Find out max in group
        max_in_group=[]
        for position, current_count in position_counts.items():
            if len(max_in_group) == 0:
                max_in_group = current_count
            else:
                max_in_group = np.maximum(max_in_group, current_count)
        for grp_idx, grp in enumerate(sample_group_ordered):
            #current_group_colour=colour_choices[grp_idx%25]
            axarr[grp_idx].set_ylim([0.5, (max_in_group[grp_idx] + max_in_group[grp_idx]/20)])
            axarr[grp_idx].set_title(grp)
            for position, current_count, in position_counts.items():
                if current_count[grp_idx] > 0:
                    axarr[grp_idx].add_patch(
                        patches.Rectangle((position-0.5,0.5),   # this is the x/y coordinates of the bottom left corner
                        1, # width of the box
                        current_count[grp_idx], # height of the box
                        #alpha=0.3, # transparency
                        linestyle=None, 
                        #color=current_group_colour)
                        color="grey")
                    )
                axarr[grp_idx].semilogy()
                axarr[grp_idx].set_ylim(ymin=1)
                axarr[grp_idx].yaxis.set_major_formatter(ScalarFormatter())
            #axarr[grp_idx].set_yscale("log", nonposy='clip')
        plt.setp(axarr[-1].get_xticklabels(), visible=False)
        plt.setp(axarr[-2].get_xticklabels(), visible=True)
        axarr[-2].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
        axarr[-1].spines['top'].set_visible(False)
        axarr[-1].spines['right'].set_visible(False)
        axarr[-1].spines['left'].set_visible(False)
        axarr[-1].spines['bottom'].set_visible(False)
        axarr[-1].axes.get_yaxis().set_visible(False)
        fig.savefig(output_fp)
        plt.close(fig)
    # This function takes in the cache folder and prints out the output files
    # This print_output is specified for 'no_blast' unification
    def print_output(self, gtf_obj, cache_dir, out_dir):
        self.make_no_blast_network(gtf_obj = gtf_obj)
        self.annotate_transcript_model(gtf_obj = gtf_obj)
        self.remove_excess_nodes()
        self.print_node_annotation(cache_dir=cache_dir)
        self.print_sample_level_node_count(cache_dir=cache_dir, out_dir= out_dir) if len(self.sample_annotation.group_order_idx) > 1 else 0
        self.print_gml(cache_dir=cache_dir, out_dir= out_dir)
        self.print_matplotlib_graph(cache_dir=cache_dir, gtf_obj = gtf_obj)
